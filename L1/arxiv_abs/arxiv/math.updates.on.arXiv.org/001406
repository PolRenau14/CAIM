This paper studies Bayesian ranking and selection (R&S) problems with correlated prior beliefs
and continuous domains, i.e. Bayesian optimization (BO). Knowledge gradient methods [Frazier
et al., 2008, 2009] have been widely studied for discrete R&S problems, which sample the one-step
Bayes-optimal point. When used over continuous domains, previous work on the knowledge gradient
[Scott et al., 2011, Wu and Frazier, 2016, Wu et al., 2017] often rely on a discretized finite approximation.
However, the discretization introduces error and scales poorly as the dimension of domain grows.
In this paper, we develop a fast discretization-free knowledge gradient method for Bayesian optimization.
Our method is not restricted to the fully sequential setting, but useful in all settings where knowledge
gradient can be used over continuous domains. We show how our method can be generalized to handle
(i) batch of points suggestion (parallel knowledge gradient); (ii) the setting where derivative
information is available in the optimization process (derivative-enabled knowledge gradient).
In numerical experiments, we demonstrate that the discretization-free knowledge gradient method
finds global optima significantly faster than previous Bayesian optimization algorithms on both
synthetic test functions and real-world applications, especially when function evaluations
are noisy; and derivative-enabled knowledge gradient can further improve the performances, even
outperforming the gradient-based optimizer such as BFGS when derivative information is available.
