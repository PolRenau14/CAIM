Forecasting time series data is an important subject in economics, business, and finance. Traditionally,
there are several techniques to effectively forecast the next lag of time series data such as univariate
Autoregressive (AR), univariate Moving Average (MA), Simple Exponential Smoothing (SES), and
more notably Autoregressive Integrated Moving Average (ARIMA) with its many variations. In particular,
ARIMA model has demonstrated its outperformance in precision and accuracy of predicting the next
lags of time series. With the recent advancement in computational power of computers and more importantly
developing more advanced machine learning algorithms and approaches such as deep learning, new
algorithms are developed to forecast time series data. The research question investigated in this
article is that whether and how the newly developed deep learning-based algorithms for forecasting
time series data, such as "Long Short-Term Memory (LSTM)", are superior to the traditional algorithms.
The empirical studies conducted and reported in this article show that deep learning-based algorithms
such as LSTM outperform traditional-based algorithms such as ARIMA model. More specifically,
the average reduction in error rates obtained by LSTM is between 84 - 87 percent when compared to ARIMA
indicating the superiority of LSTM to ARIMA. Furthermore, it was noticed that the number of training
times, known as "epoch" in deep learning, has no effect on the performance of the trained forecast
model and it exhibits a truly random behavior. 