Intelligent Personal Assistant (IA), also known as Voice Assistant (VA), has become increasingly
popular as a human-computer interaction mechanism. Most smartphones have built-in voice assistants
that are granted high privilege, which is able to access system resources and private information.
Thus, once the voice assistants are exploited by attackers, they become the stepping stones for
the attackers to hack into the smartphones. Prior work shows that the voice assistant can be activated
by inter-component communication mechanism, through an official Android API. However, this attack
method is only effective on Google Assistant, which is the official voice assistant developed by
Google. Voice assistants in other operating systems, even custom Android systems, cannot be activated
by this mechanism. Prior work also shows that the attacking voice commands can be inaudible, but
it requires additional instruments to launch the attack, making it unrealistic for real-world
attack. We propose an attacking framework, which records the activation voice of the user, and launch
the attack by playing the activation voice and attack commands via the built-in speaker. An intelligent
stealthy module is designed to decide on the suitable occasion to launch the attack, preventing
the attack being noticed by the user. We demonstrate proof-of-concept attacks on Google Assistant,
showing the feasibility and stealthiness of the proposed attack scheme. We suggest to revise the
activation logic of voice assistant to be resilient to the speaker based attack. 