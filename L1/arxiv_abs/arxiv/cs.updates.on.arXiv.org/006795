One of the main computational and scientific challenges in the modern age is to extract useful information
from unstructured texts. Topic models are one popular machine-learning approach which infers
the latent topical structure of a collection of documents. Despite their success --- in particular
of its most widely used variant called Latent Dirichlet Allocation (LDA) --- and numerous applications
in sociology, history, and linguistics, topic models are known to suffer from severe conceptual
and practical problems, e.g. a lack of justification for the Bayesian priors, discrepancies with
statistical properties of real texts, and the inability to properly choose the number of topics.
Here we obtain a fresh view on the problem of identifying topical structures by relating it to the
problem of finding communities in complex networks. This is achieved by representing text corpora
as bipartite networks of documents and words. By adapting existing community-detection methods
-- using a stochastic block model (SBM) with non-parametric priors -- we obtain a more versatile
and principled framework for topic modeling (e.g., it automatically detects the number of topics
and hierarchically clusters both the words and documents). The analysis of artificial and real
corpora demonstrates that our SBM approach leads to better topic models than LDA in terms of statistical
model selection. More importantly, our work shows how to formally relate methods from community
detection and topic modeling, opening the possibility of cross-fertilization between these two
fields. 