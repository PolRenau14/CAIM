In light of continued advances in loop scheduling, this work revisits the OpenMP loop scheduling
by outlining the current state of the art in loop scheduling and presenting evidence that the existing
OpenMP schedules are insufficient for all combinations of applications, systems, and their characteristics.
A review of the state of the art shows that due to the specifics of the parallel applications, the variety
of computing platforms, and the numerous performance degradation factors, no single loop scheduling
technique can be a 'one-fits-all' solution to effectively optimize the performance of all parallel
applications in all situations. The impact of irregularity in computational workloads and hardware
systems, including operating system noise, on the performance of parallel applications, results
in performance loss and has often been neglected in loop scheduling research, in particular, the
context of OpenMP schedules. Existing dynamic loop self-scheduling techniques, such as trapezoid
self-scheduling, factoring, and weighted factoring, offer an unexplored potential to alleviate
this degradation in OpenMP due to the fact that they explicitly target the minimization of load imbalance
and scheduling overhead. Through theoretical and experimental evaluation, this work shows that
these loop self-scheduling methods provide a benefit in the context of OpenMP. In conclusion, OpenMP
must include more schedules to offer a broader performance coverage of applications executing
on an increasing variety of heterogeneous shared memory computing platforms. 