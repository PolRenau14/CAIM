An important task for a recommender system to provide interpretable explanations for the user.
This is important for the credibility of the system. Current interpretable recommender systems
tend to focus on certain features known to be important to the user and offer their explanations in
a structured form. It is well known that user generated reviews and feedback from reviewers have
strong leverage over the users' decisions. On the other hand, recent text generation works have
been shown to generate text of similar quality to human written text, and we aim to show that generated
text can be successfully used to explain recommendations. In this paper, we propose a framework
consisting of popular review-oriented generation models aiming to create personalised explanations
for recommendations. The interpretations are generated at both character and word levels. We build
a dataset containing reviewers' feedback from the Amazon books review dataset. Our cross-domain
experiments are designed to bridge from natural language processing to the recommender system
domain. Besides language model evaluation methods, we employ DeepCoNN, a novel review-oriented
recommender system using a deep neural network, to evaluate the recommendation performance of
generated reviews by root mean square error (RMSE). We demonstrate that the synthetic personalised
reviews have better recommendation performance than human written reviews. To our knowledge,
this presents the first machine-generated natural language explanations for rating prediction.
