Accurate face recognition techniques make a series of critical applications possible: policemen
could employ it to retrieve criminals' faces from surveillance video streams; cross boarder travelers
could pass a face authentication inspection line without the involvement of officers. Nonetheless,
when public security heavily relies on such intelligent systems, the designers should deliberately
consider the emerging attacks aiming at misleading those systems employing face recognition.
We propose a kind of brand new attack against face recognition systems, which is realized by illuminating
the subject using infrared according to the adversarial examples worked out by our algorithm, thus
face recognition systems can be bypassed or misled while simultaneously the infrared perturbations
cannot be observed by raw eyes. Through launching this kind of attack, an attacker not only can dodge
surveillance cameras. More importantly, he can impersonate his target victim and pass the face
authentication system, if only the victim's photo is acquired by the attacker. Again, the attack
is totally unobservable by nearby people, because not only the light is invisible, but also the device
we made to launch the attack is small enough. According to our study on a large dataset, attackers
have a very high success rate with a over 70\% success rate for finding such an adversarial example
that can be implemented by infrared. To the best of our knowledge, our work is the first one to shed
light on the severity of threat resulted from infrared adversarial examples against face recognition.
