Recent advances in consumer depth sensors have created many opportunities for human body measurement
and modeling. Estimation of 3D body shape is particularly useful for fashion e-commerce applications
such as virtual try-on or fit personalization. In this paper, we propose a method for capturing accurate
human body shape and anthropometrics from a single consumer grade depth sensor. We first generate
a large dataset of synthetic 3D human body models using real-world body size distributions. Next,
we estimate key body measurements from a single monocular depth image. We combine body measurement
estimates with local geometry features around key joint positions to form a robust multi-dimensional
feature vector. This allows us to conduct a fast nearest-neighbor search to every sample in the dataset
and return the closest one. Compared to existing methods, our approach is able to predict accurate
full body parameters from a partial view using measurement parameters learned from the synthetic
dataset. Furthermore, our system is capable of generating 3D human mesh models in real-time, which
is significantly faster than methods which attempt to model shape and pose deformations. To validate
the efficiency and applicability of our system, we collected a dataset that contains frontal and
back scans of 83 clothed people with ground truth height and weight. Experiments on real-world dataset
show that the proposed method can achieve real-time performance with competing results achieving
an average error of 1.9 cm in estimated measurements. 