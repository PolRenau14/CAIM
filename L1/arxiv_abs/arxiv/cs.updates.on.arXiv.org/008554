The optimization of high dimensional functions is a key issue in engineering problems but it frequently
comes at a cost that is not acceptable since it usually involves a complex and expensive computer
code. Engineers often overcome this limitation by first identifying which parameters drive the
most the function variations: non-influential variables are set to a fixed value and the optimization
procedure is carried out with the remaining influential variables. Such variable selection is
performed through influence measures that are meaningful for regression problems. However it
does not account for the specific structure of optimization problems where we would like to identify
which variables most lead to constraints satisfaction and low values of the objective function.
In this paper, we propose a new sensitivity analysis that accounts for the specific aspects of optimization
problems. In particular, we introduce an influence measure based on the Hilbert-Schmidt Independence
Criterion to characterize whether a design variable matters to reach low values of the objective
function and to satisfy the constraints. This sensitivity measure makes it possible to sort the
inputs and reduce the problem dimension. We compare a random and a greedy strategies to set the values
of the non-influential variables before conducting a local optimization. Applications to several
test-cases show that this variable selection and the greedy strategy significantly reduce the
number of function evaluations at a limited cost in terms of solution performance. 