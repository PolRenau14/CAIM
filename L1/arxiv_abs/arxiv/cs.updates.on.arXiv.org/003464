The concentration of measure phenomena were discovered as the mathematical background of statistical
mechanics at the end of the XIX - beginning of the XX century and were then explored in mathematics
of the XX-XXI centuries. At the beginning of the XXI century, it became clear that the proper utilisation
of these phenomena in machine learning might transform the curse of dimensionality into the blessing
of dimensionality. This paper summarises recently discovered phenomena of measure concentration
which drastically simplify some machine learning problems in high dimension, and allow us to correct
legacy artificial intelligence systems. The classical concentration of measure theorems state
that i.i.d. random points are concentrated in a thin layer near a surface (a sphere or equators of
a sphere, an average or median level set of energy or another Lipschitz function, etc.). The new stochastic
separation theorems describe the thin structure of these thin layers: the random points are not
only concentrated in a thin layer but are all linearly separable from the rest of the set, even for
exponentially large random sets. The linear functionals for separation of points can be selected
in the form of the linear Fisher's discriminant. All artificial intelligence systems make errors.
Non-destructive correction requires separation of the situations (samples) with errors from
the samples corresponding to correct behaviour by a simple and robust classifier. The stochastic
separation theorems provide us by such classifiers and a non-iterative (one-shot) procedure for
learning. 