In this paper, we consider a modified version of the control problem in a model free Markov decision
process (MDP) setting with large state and action spaces. The control problem most commonly addressed
in the contemporary literature is to find an optimal policy which maximizes the value function,
i.e., the long run discounted reward of the MDP. The current settings also assume access to a generative
model of the MDP with the hidden premise that observations of the system behaviour in the form of sample
trajectories can be obtained with ease from the model. In this paper, we consider a modified version,
where the cost function is the expectation of a non-convex function of the value function without
access to the generative model. Rather, we assume that a sample trajectory generated using a priori
chosen behaviour policy is made available. In this restricted setting, we solve the modified control
problem in its true sense, i.e., to find the best possible policy given this limited information.
We propose a stochastic approximation algorithm based on the well-known cross entropy method which
is data (sample trajectory) efficient, stable, robust as well as computationally and storage efficient.
We provide a proof of convergence of our algorithm to a policy which is globally optimal relative
to the behaviour policy. We also present experimental results to corroborate our claims and we demonstrate
the superiority of the solution produced by our algorithm compared to the state-of-the-art algorithms
under appropriately chosen behaviour policy. 