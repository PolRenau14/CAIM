Rank minimization methods have attracted considerable interest in various areas, such as computer
vision and machine learning. The most representative work is nuclear norm minimization (NNM),
which can recover the matrix rank exactly under some restricted and theoretical guarantee conditions.
However, for many real applications, NNM is not able to approximate the matrix rank accurately,
since it often tends to over-shrink the rank components. To rectify the weakness of NNM, recent advances
have shown that weighted nuclear norm minimization (WNNM) can achieve a better matrix rank approximation
than NNM, which heuristically set the weight being inverse to the singular values. However, it still
lacks a sound mathematical explanation on why WNNM is more feasible than NNM. In this paper, we propose
a scheme to analyze WNNM and NNM from the perspective of the group sparse representation. Specifically,
we design an adaptive dictionary to bridge the gap between the group sparse representation and the
rank minimization models. Based on this scheme, we provide a mathematical derivation to explain
why WNNM is more feasible than NNM. Moreover, due to the heuristical set of the weight, WNNM sometimes
pops out error in the operation of SVD, and thus we present an adaptive weight setting scheme to avoid
this error. We then employ the proposed scheme on two low-level vision tasks including image denoising
and image inpainting. Experimental results demonstrate that WNNM is more feasible than NNM and
the proposed scheme outperforms many current state-of-the-art methods. 