Representing a continuous-time signal by a set of samples is a classical problem in signal processing.
We study this problem under the additional constraint that the samples are quantized or compressed
in a lossy manner under a limited bitrate budget. To this end, we consider a combined sampling and
source coding problem in which an analog stationary Gaussian signal is reconstructed from its encoded
samples. These samples are obtained by a set of bounded linear functionals of the continuous-time
path, with a limitation on the average number of samples obtained per unit time available in this
setting. We provide a full characterization of the minimal distortion in terms of the sampling frequency,
the bitrate, and the signal's spectrum. Assuming that the signal's energy is not uniformly distributed
over its spectral support, we show that for each compression bitrate there exists a critical sampling
frequency smaller than the Nyquist rate, such that the distortion in signal reconstruction when
sampling at this frequency is minimal. Our results can be seen as an extension of the classical sampling
theorem for bandlimited random processes in the sense that it describes the minimal amount of excess
distortion in the reconstruction due to lossy compression of the samples, and provides the minimal
sampling frequency required in order to achieve this distortion. Finally, we compare the fundamental
limits in the combined source coding and sampling problem to the performance of pulse code modulation
(PCM), where each sample is quantized by a scalar quantizer using a fixed number of bits. 