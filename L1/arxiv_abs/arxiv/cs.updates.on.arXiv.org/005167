Adversaries leverage social network friend relationships to collect sensitive data from users
and target them with abuse that includes fake news, cyberbullying, malware, and propaganda. Case
in point, 71 out of 80 user study participants had at least 1 Facebook friend with whom they never interact,
either in Facebook or in real life, or whom they believe is likely to abuse their posted photos or status
updates, or post offensive, false or malicious content. We introduce AbuSniff, a system that identifies
Facebook friends perceived as strangers or abusive, and protects the user by unfriending, unfollowing,
or restricting the access to information for such friends. We develop a questionnaire to detect
perceived strangers and friend abuse.We introduce mutual Facebook activity features and show
that they can train supervised learning algorithms to predict questionnaire responses. We have
evaluated AbuSniff through several user studies with a total of 263 participants from 25 countries.
After answering the questionnaire, participants agreed to unfollow and restrict abusers in 91.6%
and 90.9% of the cases respectively, and sandbox or unfriend non-abusive strangers in 92.45% of
the cases. Without answering the questionnaire, participants agreed to take the AbuSniff suggested
action against friends predicted to be strangers or abusive, in 78.2% of the cases. AbuSniff increased
the participant self-reported willingness to reject invitations from strangers and abusers,
their awareness of friend abuse implications and their perceived protection from friend abuse.
