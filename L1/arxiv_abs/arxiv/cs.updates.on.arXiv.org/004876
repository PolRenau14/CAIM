One of the open challenges in designing robots that operate successfully in the unpredictable human
environment is how to make them able to predict what actions they can perform on objects, and what
their effects will be, i.e., the ability to perceive object affordances. Since modeling all the
possible world interactions is unfeasible, learning from experience is required, posing the challenge
of collecting a large amount of experiences (i.e., training data). Typically, a manipulative robot
operates on external objects by using its own hands (or similar end-effectors), but in some cases
the use of tools may be desirable, nevertheless, it is reasonable to assume that while a robot can
collect many sensorimotor experiences using its own hands, this cannot happen for all possible
human-made tools. Therefore, in this paper we investigate the developmental transition from hand
to tool affordances: what sensorimotor skills that a robot has acquired with its bare hands can be
employed for tool use? By employing a visual and motor imagination mechanism to represent different
hand postures compactly, we propose a probabilistic model to learn hand affordances, and we show
how this model can generalize to estimate the affordances of previously unseen tools, ultimately
supporting planning, decision-making and tool selection tasks in humanoid robots. We present
experimental results with the iCub humanoid robot, and we publicly release the collected sensorimotor
data in the form of a hand posture affordances dataset. 