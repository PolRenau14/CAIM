Sonography synthesis has a wide range of applications, including medical procedure simulation,
clinical training and multimodality image registration. In this paper, we propose a machine learning
approach to simulate ultrasound images at given 3D spatial locations (relative to the patient anatomy),
based on conditional generative adversarial networks (GANs). In particular, we introduce a novel
neural network architecture that can sample anatomically accurate images conditionally on spatial
position of the (real or mock) freehand ultrasound probe. To ensure an effective and efficient spatial
information assimilation, the proposed spatially-conditioned GANs take calibrated pixel coordinates
in global physical space as conditioning input, and utilise residual network units and shortcuts
of conditioning data in the GANs' discriminator and generator, respectively. Using optically
tracked B-mode ultrasound images, acquired by an experienced sonographer on a fetus phantom, we
demonstrate the feasibility of the proposed method by two sets of quantitative results: distances
were calculated between corresponding anatomical landmarks identified in the held-out ultrasound
images and the simulated data at the same locations unseen to the networks; a usability study was
carried out to distinguish the simulated data from the real images. In summary, we present what we
believe are state-of-the-art visually realistic ultrasound images, simulated by the proposed
GAN architecture that is stable to train and capable of generating plausibly diverse image samples.
