To enable a dense integration of model synapses in a spiking neural networks hardware, various nano-scale
devices are being considered. Such a device, besides exhibiting spike-time dependent plasticity
(STDP), needs to be highly scalable, have a large endurance and require low energy for transitioning
between states. In this work, we first introduce and empirically determine two new specifications
for an synapse in SNNs: number of conductance levels per synapse and maximum learning-rate. To the
best of our knowledge, there are no RRAMs that meet the latter specification. As a solution, we propose
the use of multiple PCMO-RRAMs in parallel within a synapse. While synaptic reading, all PCMO-RRAMs
are simultaneously read and for each synaptic conductance-change event, the mechanism for conductance
STDP is initiated for only one RRAM, randomly picked from the set. Second, to validate our solution,
we experimentally demonstrate STDP of conductance of a PCMO-RRAM and then show that due to a large
learning-rate, a single PCMO-RRAM fails to model a synapse in the training of an SNN. As anticipated,
network training improves as more PCMO-RRAMs are added to the synapse. Fourth, we discuss the circuit-requirements
for implementing such a scheme, to conclude that the requirements are within bounds. Thus, our work
presents specifications for synaptic devices in trainable SNNs, indicates the shortcomings of
state-of-art synaptic contenders, and provides a solution to extrinsically meet the specifications
and discusses the peripheral circuitry that implements the solution. 