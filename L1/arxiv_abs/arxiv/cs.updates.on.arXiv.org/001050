One of the foundations of science is that researchers must publish the methodology used to achieve
their results so that others can attempt to reproduce them. This has the added benefit of allowing
methods to be adopted and adapted for other purposes. In the field of e-Science, services -- often
choreographed through workflow, process data to generate results. The reproduction of results
is often not straightforward as the computational objects may not be made available or may have been
updated since the results were generated. For example, services are often updated to fix bugs or
improve algorithms. This paper addresses these problems in three ways. Firstly, it introduces
a new framework to clarify the range of meanings of "reproducibility". Secondly, it describes a
new algorithm, \PDIFF, that uses a comparison of workflow provenance traces to determine whether
an experiment has been reproduced; the main innovation is that if this is not the case then the specific
point(s) of divergence are identified through graph analysis, assisting any researcher wishing
to understand those differences. One key feature is support for user-defined, semantic data comparison
operators. Finally, the paper describes an implementation of \PDIFF that leverages the power of
the e-Science Central platform which enacts workflows in the cloud. As well as automatically generating
a provenance trace for consumption by \PDIFF, the platform supports the storage and re-use of old
versions of workflows, data and services; the paper shows how this can be powerfully exploited in
order to achieve reproduction and re-use. 