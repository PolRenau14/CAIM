It is unknown what kind of biases modern in the wild face datasets have because of their lack of annotation.
A direct consequence of this is that total recognition rates alone only provide limited insight
about the generalization ability of a Deep Convolutional Neural Networks (DCNNs). We propose to
empirically study the effect of different types of dataset biases on the generalization ability
of DCNNs. Using synthetically generated face images, we study the face recognition rate as a function
of interpretable parameters such as face pose and light. The proposed method allows valuable details
about the generalization performance of different DCNN architectures to be observed and compared.
In our experiments, we find that: 1) Indeed, dataset bias has a significant influence on the generalization
performance of DCNNs. 2) DCNNs can generalize surprisingly well to unseen illumination conditions
and large sampling gaps in the pose variation. 3) Using the presented methodology we reveal that
the VGG-16 architecture outperforms the AlexNet architecture at face recognition tasks because
it can much better generalize to unseen face poses, although it has significantly more parameters.
4) We uncover a main limitation of current DCNN architectures, which is the difficulty to generalize
when different identities to not share the same pose variation. 5) We demonstrate that our findings
on synthetic data also apply when learning from real-world data. Our face image generator is publicly
available to enable the community to benchmark other DCNN architectures. 