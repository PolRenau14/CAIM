A fundamental challenge in developing high-impact machine learning technologies is balancing
the need to model rich, structured domains with the ability to scale to big data. Many important problem
areas are both richly structured and large scale, from social and biological networks, to knowledge
graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms
for modeling structured data, and show that they can both capture rich structure and scale to big
data. The first, hinge-loss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical
model that generalizes different approaches to convex inference. We unite three approaches from
the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing
that all three lead to the same inference objective. We then define HL-MRFs by generalizing this
unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic
programming language that makes HL-MRFs easy to define using a syntax based on first-order logic.
We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that
is much more scalable than general-purpose convex optimization methods, because it uses message
passing to take advantage of sparse dependency structures. We then show how to learn the parameters
of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable.
Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously
possible. 