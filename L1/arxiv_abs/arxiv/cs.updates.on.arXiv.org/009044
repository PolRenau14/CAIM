In real-world applications, it is often expensive and time-consuming to obtain labeled examples.
In such cases, knowledge transfer from related domains, where labels are abundant, could greatly
reduce the need for extensive labeling efforts. In this scenario, transfer learning comes in hand.
In this paper, we propose Deep Variational Transfer (DVT), a variational autoencoder that transfers
knowledge across domains using a shared latent Gaussian mixture model. Thanks to the combination
of a semi-supervised ELBO and parameters sharing across domains, we are able to simultaneously:
(i) align all supervised examples of the same class into the same latent Gaussian Mixture component,
independently from their domain; (ii) predict the class of unsupervised examples from different
domains and use them to better model the occurring shifts. We perform tests on MNIST and USPS digits
datasets, showing DVT's ability to perform transfer learning across heterogeneous datasets.
Additionally, we present DVT's top classification performances on the MNIST semi-supervised
learning challenge. We further validate DVT on a astronomical datasets. DVT achieves states-of-the-art
classification performances, transferring knowledge across real stars surveys datasets, EROS,
MACHO and HiTS, . In the worst performance, we double the achieved F1-score for rare classes. These
experiments show DVT's ability to tackle all major challenges posed by transfer learning: different
covariate distributions, different and highly imbalanced class distributions and different
feature spaces. 