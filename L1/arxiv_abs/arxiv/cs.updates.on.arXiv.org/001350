Given a set $F$ of $n$ positive functions over a ground set $X$, we consider the problem of computing
$x^*$ that minimizes the expression $\sum_{f\in F}f(x)$, over $x\in X$. A typical application
is \emph{shape fitting}, where we wish to approximate a set $P$ of $n$ elements (say, points) by a
shape $x$ from a (possibly infinite) family $X$ of shapes. Here, each point $p\in P$ corresponds
to a function $f$ such that $f(x)$ is the distance from $p$ to $x$, and we seek a shape $x$ that minimizes
the sum of distances from each point in $P$. In the $k$-clustering variant, each $x\in X$ is a tuple
of $k$ shapes, and $f(x)$ is the distance from $p$ to its closest shape in $x$. Our main result is a unified
framework for constructing {\em coresets} and {\em approximate clustering} for such general sets
of functions. To achieve our results, we forge a link between the classic and well defined notion
of $\varepsilon$-approximations from the theory of PAC Learning and VC dimension, to the relatively
new (and not so consistent) paradigm of coresets, which are some kind of "compressed representation"
of the input set $F$. Using traditional techniques, a coreset usually implies an LTAS (linear time
approximation scheme) for the corresponding optimization problem, which can be computed in parallel,
via one pass over the data, and using only polylogarithmic space (i.e, in the streaming model). We
show how to generalize the results of our framework for squared distances (as in $k$-mean), distances
to the $q$th power, and deterministic constructions. 