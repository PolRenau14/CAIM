Music is usually highly structured and it is still an open question how to design models which can
successfully learn to recognize and represent musical structure. A fundamental problem is that
structurally related patterns can have very distinct appearances, because the structural relationships
are often based on transformations of musical material, like chromatic or diatonic transposition,
inversion, retrograde, or rhythm change. In this preliminary work, we study the potential of two
unsupervised learning techniques - Restricted Boltzmann Machines (RBMs) and Gated Autoencoders
(GAEs) - to capture pre-defined transformations from constructed data pairs. We evaluate the models
by using the learned representations as inputs in a discriminative task where for a given type of
transformation (e.g. diatonic transposition), the specific relation between two musical patterns
must be recognized (e.g. an upward transposition of diatonic steps). Furthermore, we measure the
reconstruction error of models when reconstructing musical transformed patterns. Lastly, we
test the models in an analogy-making task. We find that it is difficult to learn musical transformations
with the RBM and that the GAE is much more adequate for this task, since it is able to learn representations
of specific transformations that are largely content-invariant. We believe these results show
that models such as GAEs may provide the basis for more encompassing music analysis systems, by endowing
them with a better understanding of the structures underlying music. 